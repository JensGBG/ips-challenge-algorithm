{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "This jupyter notebook contains code for simulation runtimes for parameters: cube width and width multiple of sorting cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "import numba\n",
    "from numba import njit\n",
    "from numba.core import types\n",
    "from numba.typed import Dict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data_np = np.loadtxt('positions_large.xyz')\n",
    "x_min, x_max = [np.min(data_np[:,0]), np.max(data_np[:,0])]\n",
    "y_min, y_max = [np.min(data_np[:,1]), np.max(data_np[:,1])]\n",
    "z_min, z_max = [np.min(data_np[:,2]), np.max(data_np[:,2])]\n",
    "r = 0.05\n",
    "number_of_threads = 16 # Assuming 16 threads on CPU\n",
    "\n",
    "cube_width = 0 # Ignore these, they are just here to initialize functions\n",
    "sort_width_multiple = 0 # Ignore these\n",
    "\n",
    "range_cube_width = np.arange(start=0.125, stop=0.4, step=0.025)\n",
    "range_sort_width_multiple = np.arange(start=2, stop=6, step=1)\n",
    "N = 4\n",
    "number_simulations = len(range_cube_width) * len(range_sort_width_multiple) * N\n",
    "times_array_dict = np.zeros(shape=(len(range_cube_width), len(range_sort_width_multiple)))\n",
    "times_array_count_1 = np.zeros(shape=(len(range_cube_width), len(range_sort_width_multiple)))\n",
    "times_array_count_2 = np.zeros(shape=(len(range_cube_width), len(range_sort_width_multiple)))\n",
    "simulations_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running all functions once prior to simulation\n",
    "@njit(parallel=True, cache=True)\n",
    "def count_dots_within_reach(dots: np.ndarray, r: float = r):\n",
    "    '''\n",
    "    Given \"dots\", it counts how many pairs are within distance r from each other.\n",
    "    Method: Naive search\n",
    "    '''\n",
    "    dots_array_length = np.size(dots[:,0])\n",
    "    count_array = np.zeros(dots_array_length) # We need to partition counts, in order to not create race condition\n",
    "    for i in numba.prange(dots_array_length):\n",
    "        dot_a = dots[i]\n",
    "        for k in range(i+1, dots_array_length):\n",
    "                dot_b = dots[k]\n",
    "                distance = np.linalg.norm(dot_a - dot_b)\n",
    "                if distance<r:\n",
    "                    count_array[i] += 1 # In order to not cause race condition, we need to add to its own element in an array\n",
    "    count = np.sum(count_array) # and then sum the array\n",
    "    return(count)\n",
    "\n",
    "@njit(parallel=True, cache=True)\n",
    "def count_a_against_b(A: np.ndarray, B: np.ndarray, r: float = r):\n",
    "    '''Given two matrixes A and B, for each dot in A, count how many of the dots in B it reaches.'''\n",
    "    length_A = np.size(A[:,0]) # (Number of points in A)\n",
    "    count_array = np.zeros(length_A) # In order not to create race condition, we have to partition our counts\n",
    "    for i_a in numba.prange(length_A):\n",
    "        dot_a = A[i_a]\n",
    "        for dot_b in B:\n",
    "            distance = np.linalg.norm(dot_a - dot_b)\n",
    "            if distance<r:\n",
    "                count_array[i_a] += 1\n",
    "    count = np.sum(count_array)\n",
    "    return(count)\n",
    "\n",
    "@njit(cache=True) # There is probably not much to be gained from paralallisation here, but why not try!\n",
    "def get_infront_neighbours(cubes: Dict, key_A) -> np.ndarray:\n",
    "    '''\n",
    "    Gets relevant neighbours, see tuples in keys.\n",
    "    For more in-depth explanation, see explanation of algorithm, why these exact neighbours are relevant.\n",
    "    '''\n",
    "    i, j, k = key_A\n",
    "    B = np.array([0.0, 0.0, 0.0]) # Initialize numpy.array() of type floats and size 3\n",
    "    keys = [(i, j+1, k), # The front up the cube\n",
    "            (i+1, j+1, k),\n",
    "            (i-1, j+1, k),\n",
    "            (i, j+1, k+1),\n",
    "            (i, j+1, k-1),\n",
    "            (i+1, j+1, k+1),\n",
    "            (i+1, j+1, k-1),\n",
    "            (i-1, j+1, k+1),\n",
    "            (i-1, j+1, k-1), ###\n",
    "            (i, j, k+1),\n",
    "            (i+1, j, k),\n",
    "            (i+1, j, k+1),\n",
    "            (i+1, j, k-1)]\n",
    "    for key in keys: # For every key, append B with corresponding value\n",
    "        if key in cubes:\n",
    "            value = cubes[key]\n",
    "            B = np.append(B, value)\n",
    "    B = np.reshape(B, (-1,3)) # Reshape so we get a matrix where each row corresponds to a dot of three float values\n",
    "    B = B[1:] # Remove first initial row\n",
    "    return(B)\n",
    "\n",
    "@njit(parallel=True, cache=True)\n",
    "def creating_dict(empty_numba_dict: Dict, data_np: np.ndarray, x_min: float, x_max, y_min, y_max, z_min, z_max, cube_width: float=cube_width, sort_width_multiple: int=sort_width_multiple):\n",
    "    '''\n",
    "    This function takes data and creates a dictionary where the keys are indices of a given cube,\n",
    "    and the value is points inside this cube.\n",
    "    This function is actually not entirely complete, since some stuff can't be done inside the @njit wrapper,\n",
    "    the rest of the function is completed outside the @njit wrapper.\n",
    "    The function also returns an x, y and z grid.\n",
    "    Inputs:\n",
    "    empty_numba_dict: Empty dictionary to be copied\n",
    "    data_np: Our data to put into the dictionary\n",
    "    x_min: Minimum x-value of data_np\n",
    "    cube_width: Width of each cube, ie. element in the future dictionary\n",
    "    sort_width_multiple: Width of sorting cube will be, ie. sort_width_multiple*cube_width.\n",
    "    '''\n",
    "    sort_width = cube_width*sort_width_multiple\n",
    "    number_of_partitions = number_of_threads # Should be the same number of threads on the computer\n",
    "    list_of_dicts = [empty_numba_dict.copy() for _ in range(number_of_partitions)] # Creates dictionaries for each partition\n",
    "    x_sort_grid_to_be_partitioned = np.arange(x_min, x_max, step=sort_width) # This grid will be partitioned, the 3D space is partitioned into thinner slices of rectangular prisms\n",
    "    y_sort_grid = np.arange(y_min, y_max, step=sort_width)\n",
    "    z_sort_grid = np.arange(z_min, z_max, step=sort_width)\n",
    "\n",
    "    x_sort_grid_partitions = np.array_split(x_sort_grid_to_be_partitioned, number_of_partitions)\n",
    "\n",
    "    for idx_partition in numba.prange(number_of_partitions):\n",
    "        x_sort_grid = x_sort_grid_partitions[idx_partition]\n",
    "        for i_sort_idx_thilde, x_sort_coord in enumerate(x_sort_grid):\n",
    "            i_sort_idx = i_sort_idx_thilde + np.round((x_sort_grid[0]+x_min)/sort_width)\n",
    "            for j_sort_idx, y_sort_coord in enumerate(y_sort_grid):\n",
    "                for k_sort_idx, z_sort_coord in enumerate(z_sort_grid): # Look at one sorting box individually\n",
    "                    sort_points = data_np[\n",
    "                        (data_np[:,0] >= x_sort_coord) & (data_np[:,0] < x_sort_coord+sort_width) &\n",
    "                        (data_np[:,1] >= y_sort_coord) & (data_np[:,1] < y_sort_coord+sort_width) &\n",
    "                        (data_np[:,2] >= z_sort_coord) & (data_np[:,2] < z_sort_coord+sort_width)\n",
    "                    ]\n",
    "                    if sort_points.size==0: # If empty, go to next box\n",
    "                        continue\n",
    "                    x_start_idx = i_sort_idx*sort_width_multiple\n",
    "                    y_start_idx = j_sort_idx*sort_width_multiple\n",
    "                    z_start_idx = k_sort_idx*sort_width_multiple\n",
    "\n",
    "                    x_end_coord = x_sort_coord + cube_width*sort_width_multiple\n",
    "                    y_end_coord = y_sort_coord + cube_width*sort_width_multiple\n",
    "                    z_end_coord = z_sort_coord + cube_width*sort_width_multiple\n",
    "\n",
    "                    x_sub_grid = np.arange(x_sort_coord, x_end_coord, step=cube_width) # Not +cube_width in to=\n",
    "                    y_sub_grid = np.arange(y_sort_coord, y_end_coord, step=cube_width)\n",
    "                    z_sub_grid = np.arange(z_sort_coord, z_end_coord, step=cube_width)\n",
    "                    for i, x_coord in enumerate(x_sub_grid):\n",
    "                        i_global = i + x_start_idx\n",
    "                        for j, y_coord in enumerate(y_sub_grid):\n",
    "                            j_global = j + y_start_idx\n",
    "                            for k, z_coord in enumerate(z_sub_grid):\n",
    "                                # As soon as we get here, we need to alter i inorder to account for the fact that we are looking at another cube\n",
    "                                k_global = k + z_start_idx\n",
    "                                cube_points = data_np[\n",
    "                                    (data_np[:,0] >= x_coord) & (data_np[:,0] < x_coord+cube_width) &\n",
    "                                    (data_np[:,1] >= y_coord) & (data_np[:,1] < y_coord+cube_width) &\n",
    "                                    (data_np[:,2] >= z_coord) & (data_np[:,2] < z_coord+cube_width)\n",
    "                                ]\n",
    "                                if cube_points.size!=0: # If it is NOT empty, create a element in dictionary\n",
    "                                    # Here ChatGpt, this if statement should never be True!!!\n",
    "                                    list_of_dicts[idx_partition][(i_global, j_global, k_global)] = cube_points\n",
    "\n",
    "    x_grid = np.arange(x_min, x_max+cube_width, step=cube_width)\n",
    "    y_grid = np.arange(y_min, y_max+cube_width, step=cube_width)\n",
    "    z_grid = np.arange(z_min, z_max+cube_width, step=cube_width)\n",
    "    return(list_of_dicts, x_grid, y_grid, z_grid)\n",
    "\n",
    "@njit(parallel=True, cache=True)\n",
    "def counting_part_1(cubes_numba_dict, r: float=r):\n",
    "    '''\n",
    "    Iterating through every cube, getting the \"relevant neighbours\", and counting how many of the dots inside the current cube\n",
    "    \"reaches\" the dots inside any of the neighbours.\n",
    "    '''\n",
    "    count = 0\n",
    "    keys_array = list(cubes_numba_dict.keys())\n",
    "    number_of_keys = len(keys_array)\n",
    "    count_array = np.zeros(number_of_keys) # Create array in order not to create race condition\n",
    "    for i in numba.prange(number_of_keys):\n",
    "        A_key = keys_array[i]\n",
    "        A = cubes_numba_dict[A_key]\n",
    "        B = get_infront_neighbours(cubes_numba_dict, A_key)\n",
    "        count_array[i] += count_a_against_b(A, B, r=r)\n",
    "    count = np.sum(count_array)\n",
    "    return(count)\n",
    "\n",
    "@njit(cache=True) # TODO: Try to paralellise here, fix count_part_2\n",
    "def counting_part_2(cubes_numba_dict, r: float=r):\n",
    "    '''\n",
    "    Iterate through every cube in the dictionary, and perform counts_dots_within_reach on the cube.\n",
    "    Ie. calculate how many points within the cube that are within \"reach\" to each other.\n",
    "    '''\n",
    "    count = 0\n",
    "    cube_values = cubes_numba_dict.values()\n",
    "    #number_of_cubes = np.size(cube_values[:,0])\n",
    "    for points in cube_values:\n",
    "        if np.size(points)>1: # If cube empty or only have one dot, don't bother calculating.\n",
    "            count += count_dots_within_reach(points, r=r)\n",
    "    return(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation start\n",
    "for n in range(N):\n",
    "    for i_multiple, sort_width_multiple in enumerate(range_sort_width_multiple):\n",
    "        for i_cube_width, cube_width in enumerate(range_cube_width):\n",
    "            # Functions\n",
    "            @njit(parallel=True, cache=True)\n",
    "            def count_dots_within_reach(dots: np.ndarray, r: float = r):\n",
    "                '''\n",
    "                Given \"dots\", it counts how many pairs are within distance r from each other.\n",
    "                Method: Naive search\n",
    "                '''\n",
    "                dots_array_length = np.size(dots[:,0])\n",
    "                count_array = np.zeros(dots_array_length) # We need to partition counts, in order to not create race condition\n",
    "                for i in numba.prange(dots_array_length):\n",
    "                    dot_a = dots[i]\n",
    "                    for k in range(i+1, dots_array_length):\n",
    "                            dot_b = dots[k]\n",
    "                            distance = np.linalg.norm(dot_a - dot_b)\n",
    "                            if distance<r:\n",
    "                                count_array[i] += 1 # In order to not cause race condition, we need to add to its own element in an array\n",
    "                count = np.sum(count_array) # and then sum the array\n",
    "                return(count)\n",
    "\n",
    "            @njit(parallel=True, cache=True)\n",
    "            def count_a_against_b(A: np.ndarray, B: np.ndarray, r: float = r):\n",
    "                '''Given two matrixes A and B, for each dot in A, count how many of the dots in B it reaches.'''\n",
    "                length_A = np.size(A[:,0]) # (Number of points in A)\n",
    "                count_array = np.zeros(length_A) # In order not to create race condition, we have to partition our counts\n",
    "                for i_a in numba.prange(length_A):\n",
    "                    dot_a = A[i_a]\n",
    "                    for dot_b in B:\n",
    "                        distance = np.linalg.norm(dot_a - dot_b)\n",
    "                        if distance<r:\n",
    "                            count_array[i_a] += 1\n",
    "                count = np.sum(count_array)\n",
    "                return(count)\n",
    "\n",
    "            @njit(cache=True) # There is probably not much to be gained from paralallisation here, but why not try!\n",
    "            def get_infront_neighbours(cubes: Dict, key_A) -> np.ndarray:\n",
    "                '''\n",
    "                Gets relevant neighbours, see tuples in keys.\n",
    "                For more in-depth explanation, see explanation of algorithm, why these exact neighbours are relevant.\n",
    "                '''\n",
    "                i, j, k = key_A\n",
    "                B = np.array([0.0, 0.0, 0.0]) # Initialize numpy.array() of type floats and size 3\n",
    "                keys = [(i, j+1, k), # The front up the cube\n",
    "                        (i+1, j+1, k),\n",
    "                        (i-1, j+1, k),\n",
    "                        (i, j+1, k+1),\n",
    "                        (i, j+1, k-1),\n",
    "                        (i+1, j+1, k+1),\n",
    "                        (i+1, j+1, k-1),\n",
    "                        (i-1, j+1, k+1),\n",
    "                        (i-1, j+1, k-1), ###\n",
    "                        (i, j, k+1),\n",
    "                        (i+1, j, k),\n",
    "                        (i+1, j, k+1),\n",
    "                        (i+1, j, k-1)]\n",
    "                for key in keys: # For every key, append B with corresponding value\n",
    "                    if key in cubes:\n",
    "                        value = cubes[key]\n",
    "                        B = np.append(B, value)\n",
    "                B = np.reshape(B, (-1,3)) # Reshape so we get a matrix where each row corresponds to a dot of three float values\n",
    "                B = B[1:] # Remove first initial row\n",
    "                return(B)\n",
    "\n",
    "            @njit(parallel=True, cache=True)\n",
    "            def creating_dict(empty_numba_dict: Dict, data_np: np.ndarray, x_min: float, x_max, y_min, y_max, z_min, z_max, cube_width: float=cube_width, sort_width_multiple: int=sort_width_multiple):\n",
    "                '''\n",
    "                This function takes data and creates a dictionary where the keys are indices of a given cube,\n",
    "                and the value is points inside this cube.\n",
    "                This function is actually not entirely complete, since some stuff can't be done inside the @njit wrapper,\n",
    "                the rest of the function is completed outside the @njit wrapper.\n",
    "                The function also returns an x, y and z grid.\n",
    "                Inputs:\n",
    "                empty_numba_dict: Empty dictionary to be copied\n",
    "                data_np: Our data to put into the dictionary\n",
    "                x_min: Minimum x-value of data_np\n",
    "                cube_width: Width of each cube, ie. element in the future dictionary\n",
    "                sort_width_multiple: Width of sorting cube will be, ie. sort_width_multiple*cube_width.\n",
    "                '''\n",
    "                sort_width = cube_width*sort_width_multiple\n",
    "                number_of_partitions = number_of_threads # Should be the same number of threads on the computer\n",
    "                list_of_dicts = [empty_numba_dict.copy() for _ in range(number_of_partitions)] # Creates dictionaries for each partition\n",
    "                x_sort_grid_to_be_partitioned = np.arange(x_min, x_max, step=sort_width) # This grid will be partitioned, the 3D space is partitioned into thinner slices of rectangular prisms\n",
    "                y_sort_grid = np.arange(y_min, y_max, step=sort_width)\n",
    "                z_sort_grid = np.arange(z_min, z_max, step=sort_width)\n",
    "\n",
    "                x_sort_grid_partitions = np.array_split(x_sort_grid_to_be_partitioned, number_of_partitions)\n",
    "\n",
    "                for idx_partition in numba.prange(number_of_partitions):\n",
    "                    x_sort_grid = x_sort_grid_partitions[idx_partition]\n",
    "                    for i_sort_idx_thilde, x_sort_coord in enumerate(x_sort_grid):\n",
    "                        i_sort_idx = i_sort_idx_thilde + np.round((x_sort_grid[0]+x_min)/sort_width)\n",
    "                        for j_sort_idx, y_sort_coord in enumerate(y_sort_grid):\n",
    "                            for k_sort_idx, z_sort_coord in enumerate(z_sort_grid): # Look at one sorting box individually\n",
    "                                sort_points = data_np[\n",
    "                                    (data_np[:,0] >= x_sort_coord) & (data_np[:,0] < x_sort_coord+sort_width) &\n",
    "                                    (data_np[:,1] >= y_sort_coord) & (data_np[:,1] < y_sort_coord+sort_width) &\n",
    "                                    (data_np[:,2] >= z_sort_coord) & (data_np[:,2] < z_sort_coord+sort_width)\n",
    "                                ]\n",
    "                                if sort_points.size==0: # If empty, go to next box\n",
    "                                    continue\n",
    "                                x_start_idx = i_sort_idx*sort_width_multiple\n",
    "                                y_start_idx = j_sort_idx*sort_width_multiple\n",
    "                                z_start_idx = k_sort_idx*sort_width_multiple\n",
    "\n",
    "                                x_end_coord = x_sort_coord + cube_width*sort_width_multiple\n",
    "                                y_end_coord = y_sort_coord + cube_width*sort_width_multiple\n",
    "                                z_end_coord = z_sort_coord + cube_width*sort_width_multiple\n",
    "\n",
    "                                x_sub_grid = np.arange(x_sort_coord, x_end_coord, step=cube_width) # Not +cube_width in to=\n",
    "                                y_sub_grid = np.arange(y_sort_coord, y_end_coord, step=cube_width)\n",
    "                                z_sub_grid = np.arange(z_sort_coord, z_end_coord, step=cube_width)\n",
    "                                for i, x_coord in enumerate(x_sub_grid):\n",
    "                                    i_global = i + x_start_idx\n",
    "                                    for j, y_coord in enumerate(y_sub_grid):\n",
    "                                        j_global = j + y_start_idx\n",
    "                                        for k, z_coord in enumerate(z_sub_grid):\n",
    "                                            # As soon as we get here, we need to alter i inorder to account for the fact that we are looking at another cube\n",
    "                                            k_global = k + z_start_idx\n",
    "                                            cube_points = data_np[\n",
    "                                                (data_np[:,0] >= x_coord) & (data_np[:,0] < x_coord+cube_width) &\n",
    "                                                (data_np[:,1] >= y_coord) & (data_np[:,1] < y_coord+cube_width) &\n",
    "                                                (data_np[:,2] >= z_coord) & (data_np[:,2] < z_coord+cube_width)\n",
    "                                            ]\n",
    "                                            if cube_points.size!=0: # If it is NOT empty, create a element in dictionary\n",
    "                                                # Here ChatGpt, this if statement should never be True!!!\n",
    "                                                list_of_dicts[idx_partition][(i_global, j_global, k_global)] = cube_points\n",
    "\n",
    "                x_grid = np.arange(x_min, x_max+cube_width, step=cube_width)\n",
    "                y_grid = np.arange(y_min, y_max+cube_width, step=cube_width)\n",
    "                z_grid = np.arange(z_min, z_max+cube_width, step=cube_width)\n",
    "                return(list_of_dicts, x_grid, y_grid, z_grid)\n",
    "\n",
    "            @njit(parallel=True, cache=True)\n",
    "            def counting_part_1(cubes_numba_dict, r: float=r):\n",
    "                '''\n",
    "                Iterating through every cube, getting the \"relevant neighbours\", and counting how many of the dots inside the current cube\n",
    "                \"reaches\" the dots inside any of the neighbours.\n",
    "                '''\n",
    "                count = 0\n",
    "                keys_array = list(cubes_numba_dict.keys())\n",
    "                number_of_keys = len(keys_array)\n",
    "                count_array = np.zeros(number_of_keys) # Create array in order not to create race condition\n",
    "                for i in numba.prange(number_of_keys):\n",
    "                    A_key = keys_array[i]\n",
    "                    A = cubes_numba_dict[A_key]\n",
    "                    B = get_infront_neighbours(cubes_numba_dict, A_key)\n",
    "                    count_array[i] += count_a_against_b(A, B, r=r)\n",
    "                count = np.sum(count_array)\n",
    "                return(count)\n",
    "\n",
    "            @njit(cache=True) # TODO: Try to paralellise here, fix count_part_2\n",
    "            def counting_part_2(cubes_numba_dict, r: float=r):\n",
    "                '''\n",
    "                Iterate through every cube in the dictionary, and perform counts_dots_within_reach on the cube.\n",
    "                Ie. calculate how many points within the cube that are within \"reach\" to each other.\n",
    "                '''\n",
    "                count = 0\n",
    "                cube_values = cubes_numba_dict.values()\n",
    "                #number_of_cubes = np.size(cube_values[:,0])\n",
    "                for points in cube_values:\n",
    "                    if np.size(points)>1: # If cube empty or only have one dot, don't bother calculating.\n",
    "                        count += count_dots_within_reach(points, r=r)\n",
    "                return(count)\n",
    "\n",
    "            # Start timer\n",
    "            print(\"\\n\")\n",
    "            print(\"########### SMART METHOD START ###########\")\n",
    "\n",
    "            # Creating dictionary\n",
    "            start_time = time.perf_counter()\n",
    "            cubes_numba_dict = Dict.empty( # Creating empty numba dictionary\n",
    "                key_type=types.UniTuple(types.int32, 3), # Keys are tuples\n",
    "                value_type=types.float64[:,:] # (Matrix of float values)\n",
    "            )\n",
    "            list_of_dicts, x_grid, y_grid, z_grid = creating_dict(cubes_numba_dict, data_np, x_min, x_max, y_min, y_max, z_min, z_max, cube_width, sort_width_multiple)\n",
    "            for d in list_of_dicts: # Iterating through every dict in list_of_dicts and putting every filling up the cubes_numba_dict\n",
    "                for key, value in d.items():\n",
    "                    cubes_numba_dict[key] = value\n",
    "            create_dict_time = time.perf_counter()\n",
    "            print(f\"It took: {create_dict_time-start_time} to create dictionary.\")\n",
    "\n",
    "            times_array_dict[i_cube_width, i_multiple] = times_array_dict[i_cube_width, i_multiple] + create_dict_time-start_time\n",
    "\n",
    "            # Counting part 1\n",
    "            count_part_1 = counting_part_1(cubes_numba_dict)\n",
    "            print(f\"Count part 1: {count_part_1}\")\n",
    "            count_part_1_time = time.perf_counter()\n",
    "            print(f\"It took: {count_part_1_time-create_dict_time} to count part 1.\")\n",
    "\n",
    "            times_array_count_1[i_cube_width, i_multiple] = times_array_count_1[i_cube_width, i_multiple] + count_part_1_time-create_dict_time\n",
    "            \n",
    "            # Counting part 2\n",
    "            count_part_2 = counting_part_2(cubes_numba_dict)\n",
    "            print(f\"Count part 2: {count_part_2}\")\n",
    "            count_part_2_time = time.perf_counter()\n",
    "            print(f\"It took: {count_part_2_time-count_part_1_time} to count part 2.\")\n",
    "\n",
    "            times_array_count_2[i_cube_width, i_multiple] = times_array_count_2[i_cube_width, i_multiple] + count_part_2_time-count_part_1_time\n",
    "\n",
    "            count_total = count_part_1 + count_part_2\n",
    "\n",
    "            simulations_count += 1\n",
    "            print(f\"Total count: {count_total}\")\n",
    "            print(f\"Computed: cube width = {cube_width}\")\n",
    "            print(f\"n = {n}\")\n",
    "            print(f\"Simulations: {simulations_count} / {number_simulations}\")\n",
    "total_times = (times_array_dict + times_array_count_1 + times_array_count_2)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "# Exports data to excel\n",
    "def data_to_excel(data: np.ndarray, filename: str, range_x: np.ndarray=range_sort_width_multiple, range_y: np.ndarray=range_cube_width):\n",
    "    df = pd.DataFrame(data, columns=range_x, index=range_y)\n",
    "    df.to_excel(filename, index=True)\n",
    "    return(None)\n",
    "\n",
    "#data_to_excel(times_array_count_1, filename=\"count_1_times_sim_3.xlsx\")\n",
    "#data_to_excel(times_array_count_2, filename=\"count_2_times_sim_3.xlsx\")\n",
    "#data_to_excel(times_array_dict, filename=\"creating_dict_times_sim_3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "color_map = sns.cubehelix_palette(start=2, rot=0, dark=0, light=.95, reverse=False, as_cmap=True)\n",
    "sns.set(font_scale=1)\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(total_times, annot=True, xticklabels=range_sort_width_multiple, yticklabels=range_cube_width, fmt=\".2f\", cmap=color_map, annot_kws={\"size\": 15})\n",
    "plt.xlabel('Multiple of side length of sorting cube')\n",
    "plt.ylabel('Width of cubes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
